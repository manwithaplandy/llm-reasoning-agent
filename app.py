import os
import logging
import gradio as gr
from semantic_kernel import Kernel
from semantic_kernel.utils.logging import setup_logging
from semantic_kernel.agents import AgentGroupChat, ChatCompletionAgent
from semantic_kernel.agents.strategies import (
    KernelFunctionSelectionStrategy,
    KernelFunctionTerminationStrategy,
)
from semantic_kernel.connectors.ai.open_ai.services.azure_chat_completion import AzureChatCompletion
from semantic_kernel.contents.chat_message_content import ChatMessageContent
from semantic_kernel.contents.utils.author_role import AuthorRole
from semantic_kernel.functions.kernel_function_from_prompt import KernelFunctionFromPrompt

INITIAL_NAME = "Initial"
INITIAL_INSTRUCTIONS = """
You are the first point of contact for an AI assistant's chain of thought. 
A user will ask you a question. It will be your job to tell the Thinker agent that it needs to begin thinking about how to answer the user's question.
You will create a prompt for the Thinker agent to make sure the user's expectations are clear so the thinker can begin thinking.
"""

THINKER_NAME = "Thinker"
THINKER_INSTRUCTIONS = """
You are a thinker whose job it is to think about a request in a chain of thought. 
Your goal is to determine whether further reasoning is required to answer a query.
Consider the reasoning steps you have already thought about and whether you have considered all angles to give the user the best answer you can.
If you are done thinking, state that you are done. Otherwise, add the next reasoning step in your chain of thought.
You will only answer one of two ways: adding a single thought to the chain of thought, or indicating that you are done thinking.
Your thoughts should be very concise, only considering one step in a longer chain of thought. Don't chit chat or give long context to your thoughts.
"""

ANSWERER_NAME = "Answerer"
ANSWERER_INSTRUCTIONS = """
You are an answerer who answers a user's question based on your knowledge and the thoughts generated by the thinker.
Your goal is to provide an accurate and complete answer to the user's question to the best of your ability.
Using all of the information at hand, give the user a well-thought-out and well-written answer that is as complete as possible.
"""


# Set up Azure OpenAI credentials
azure_openai_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
azure_openai_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
deployment_name = "gpt-4o" 

# Set up logging
setup_logging()
logging.basicConfig(level=logging.DEBUG)

if not azure_openai_api_key or not azure_openai_endpoint:
    raise ValueError("Azure OpenAI API key or endpoint is not set in environment variables.")

# Create a Semantic Kernel instance function
def _create_kernel_with_chat_completion(service_id: str) -> Kernel:
    kernel = Kernel()
    kernel.add_service(
        AzureChatCompletion(
            deployment_name=deployment_name,
            endpoint=azure_openai_endpoint,
            api_key=azure_openai_api_key,
            api_version='2024-06-01',
            service_id=service_id
        )
    )
    return kernel

async def main(input):
    
    # Define the agents
    agent_thinker = ChatCompletionAgent(
        service_id="thinker",
        kernel=_create_kernel_with_chat_completion("thinker"),
        name=THINKER_NAME,
        instructions=THINKER_INSTRUCTIONS,
    )

    agent_answerer = ChatCompletionAgent(
        service_id="answerer",
        kernel=_create_kernel_with_chat_completion("answerer"),
        name=ANSWERER_NAME,
        instructions=ANSWERER_INSTRUCTIONS,
    )

    agent_initial = ChatCompletionAgent(
        service_id="initial",
        kernel=_create_kernel_with_chat_completion("initial"),
        name=INITIAL_NAME,
        instructions=INITIAL_INSTRUCTIONS,
    )

    termination_function = KernelFunctionFromPrompt(
        function_name="termination",
        prompt="""
        Determine if you have given a sufficient answer.  If so, respond with a single word: yes

        History:
        {{$history}}
        """,
    )

    selection_function = KernelFunctionFromPrompt(
        function_name="selection",
        prompt=f"""
        Determine which participant takes the next turn in a conversation based on the the most recent participant.
        State only the name of the participant to take the next turn.
        No participant should take more than one turn in a row.
        
        Choose only from these participants:
        - {INITIAL_NAME}
        - {THINKER_NAME}
        - {ANSWERER_NAME}
        
        Always follow these rules when selecting the next participant:
        - After user input, it is {INITIAL_NAME}'s turn.
        - {THINKER_NAME} will take multiple turns in a row until it is done thinking.
        - After {THINKER_NAME} states it is done thinking, it is {ANSWERER_NAME}'s turn.
        - {INITIAL_NAME} should only go once after the user's query, it should never be called again.

        History:
        {{{{$history}}}}
        """,
    )

    # Create the group chat using the agents and strategies outlined above
    chat = AgentGroupChat(
        agents=[agent_answerer, agent_thinker, agent_initial],
        termination_strategy=KernelFunctionTerminationStrategy(
            agents=[agent_answerer],
            function=termination_function,
            kernel=_create_kernel_with_chat_completion("termination"),
            result_parser=lambda result: str(result.value[0]).lower() == "yes",
            history_variable_name="history",
            maximum_iterations=10,
        ),
        selection_strategy=KernelFunctionSelectionStrategy(
            function=selection_function,
            kernel=_create_kernel_with_chat_completion("selection"),
            result_parser=lambda result: str(result.value[0]) if result.value is not None else ANSWERER_NAME,
            agent_variable_name="agents",
            history_variable_name="history",
        ),
    )
    
    # Output the messages in the correct order by reversing the order of the output from the generator
    async def reverse_async_gen(async_gen):
        items = []
        async for item in async_gen:
            items.append(item)
        
        for item in reversed(items):
            yield item

    async def final_output(chat: AgentGroupChat) -> str:
        output = ""
        content: ChatMessageContent
        # Collect all of the items in the generator, reverse them, and iterate them so the messages are in the correct order.
        async for content in reverse_async_gen(chat.get_chat_messages()): 
            output = output + f"## {content.name or 'User'}: \n{content.content}\n\n"
        return output

    await chat.add_chat_message(ChatMessageContent(role=AuthorRole.USER, content=input))
    print(f"# {AuthorRole.USER}: '{input}'\n")
    yield f"# {AuthorRole.USER}: \n'{input}'\n"

    async for content in chat.invoke():
        print(f"# {content.role} - {content.name or 'User'}: '{content.content}'\n")
        yield f"# {content.role} - {content.name or 'User'}: \n{content.content}\n"

    yield f"{await final_output(chat)}\n"

# Create and launch the Gradio interface
demo = gr.Interface(
    fn=main,
    inputs=gr.Textbox(lines=2, placeholder="Enter your question here..."),
    outputs=gr.Markdown(),
    title="Semantic Kernel LLM Agent with Azure OpenAI",
    description="Ask any question, and the agent will think through it step-by-step before providing a final answer.",
    allow_flagging="never"
)

demo.launch()
